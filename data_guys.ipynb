{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txp70Zk4icks"
   },
   "source": [
    "# US election Twitter mini Datathon (Advanced)\n",
    "_Classify major events based on tweets sent by users on Twitter._\n",
    "\n",
    "**Team name:** Data Guys\n",
    "\n",
    "**Team members:** Akash Gupta, Ashwin Agarwal, Kevin Biju Mathew, Shruti Mahajan\n",
    "\n",
    "\n",
    "## 1. Introduction <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "In this datathon, we aim to perform fine-grained classification and classify tweets into 4 categories: BLM, Trump, Biden, and COVID.\n",
    "\n",
    "The training data is presented in three formats: `.csv`, `.xlsx`, and `json`. We need to parse these files to get the complete training data. The testing data is available in `.csv` format.\n",
    "\n",
    "We train our supervised machine learning model as per the training data and then use the model to classify the tweets into the given categories.\n",
    "\n",
    "Overall, we perform the following tasks:\n",
    "\n",
    "1. Read all the train data and test data into their respective dataframes.\n",
    "2. Preprocess the tweets by:\n",
    "  - Decoding (to `utf-8`) tweets that are encoded.\n",
    "  - Removing non-english tweets from the training data.\n",
    "  - Tokenizing and stemming the tweets in training and testing data.\n",
    "  - Vectorizing the tweets using `TfidfVectorizer`.\n",
    "3. Finally, we create a machine learning model by fitting the vector obtained, and then classify the tweets for the testing data.\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYF1gbnCicks"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Introduction](#one)\n",
    "* [2. Import libraries](#two)\n",
    "* [3. Read and store train and test data](#three)\n",
    "* [4. Prepare data for machine learning](#four)\n",
    "* [5. Create the machine learning model](#five)\n",
    "* [6. Classify each tweet in the test data](#six)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ta65m59lickt"
   },
   "source": [
    "## 2.  Import libraries <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "In this step, we import all the libraries required for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T08:01:24.537172Z",
     "start_time": "2021-03-13T08:01:22.933237Z"
    },
    "id": "6pk_yv5Sickt"
   },
   "outputs": [],
   "source": [
    "# uncomment to install langid package if not available\n",
    "# !pip -q install langid \n",
    "\n",
    "# Importing the required libraries\n",
    "import pandas as pd   # import pandas for data frame\n",
    "from nltk import *    # import nltk for tokenizer and stemming\n",
    "from langid import classify   # used to classify the language of the tweets\n",
    "import ast, re    # used for literal evaluation and regular expressions\n",
    "from sklearn import svm   # supervised vector machine model for ML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer   # used for TFIDF vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoCDNpTUqBpL"
   },
   "source": [
    "We have successfully imported all the libraries required for our task.\n",
    "\n",
    "## 3. Read and store train and test data <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "Here we read all the files provided to us. The training data is read from `.csv`, `.xlsx`, and `.json` format files, while the test data is read from `.csv` format file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T08:11:52.008262Z",
     "start_time": "2021-03-13T08:11:51.978558Z"
    },
    "id": "_bhYhVVMseEh"
   },
   "outputs": [],
   "source": [
    "# Concatenate all the data, drop duplicates, reset index, and drop rows that contain NA\n",
    "# All the training data is read and stored in df_train dataframe\n",
    "df_train = pd.concat([pd.read_csv(\"train_csv.csv\"), \n",
    "                    pd.read_excel(\"train_excel.xlsx\"), \n",
    "                    pd.read_json(\"train_json.json\")])\\\n",
    "                    .drop_duplicates()\\\n",
    "                    .reset_index(drop = True)\\\n",
    "                    .dropna(axis = 0, how = 'any')\n",
    "\n",
    "# Loading the test data\n",
    "df_test = pd.read_csv(\"test_data_advanced.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o21xw_TNqzd4"
   },
   "source": [
    "We can successfully read the data into the respective variables.\n",
    "\n",
    "## 4. Prepare data for machine learning <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "We must preprocess the data provided to us before performing any machine learning on it. In this step, we prepare our data for machine learning.\n",
    "\n",
    "Many tweets in the train data set are in bytes format which indicates that they have been encoded. For our first step, we decode the tweets that are encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T08:11:55.995458Z",
     "start_time": "2021-03-13T08:11:55.432101Z"
    },
    "id": "GHfpYEXMicky"
   },
   "outputs": [],
   "source": [
    "def checkEncoding(tweet):\n",
    "    '''\n",
    "    In this function we use regex to find the tweets that are encoded.\n",
    "    Then, we decode those tweets.\n",
    "    param - \n",
    "    tweet: The tweets column from the dataframe.\n",
    "    return - \n",
    "    tweet: The decoded version of the tweet (if encoded). Else the original tweet.\n",
    "    '''\n",
    "    if re.match(r\"^b[\\'\\\"]\", tweet): # find tweets that start with b' or b\"\n",
    "        # Using literal to get the data in bytes so that it can be decoded\n",
    "        tweet = ast.literal_eval(tweet).decode('utf-8')\n",
    "    return tweet\n",
    "\n",
    "# Apply the checkEncoding function on each tweet of the training data\n",
    "df_train.tweet = df_train.apply(lambda x: checkEncoding(x.tweet), axis = 1)\n",
    "\n",
    "# Apply the checkEncoding function on each tweet of the test data\n",
    "df_test.tweet = df_test.apply(lambda x: checkEncoding(x.tweet), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eQ-1CXRsQP4"
   },
   "source": [
    "We have successfully decoded all the tweets in both the train and test data.\n",
    "\n",
    "Next, we remove the non-english tweets from our train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T08:12:05.569748Z",
     "start_time": "2021-03-13T08:12:05.567676Z"
    },
    "id": "edDQEmNc4k3l"
   },
   "outputs": [],
   "source": [
    "# Classify each tweet and remove tweets that are non-english\n",
    "df_en = df_train[df_train.apply(lambda x: 'en' in classify(x['tweet'])[0], axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvrPih8Bsd_2"
   },
   "source": [
    "Here we remove all tweets in the train data that are non-english.\n",
    "\n",
    "Next, we tokenize all the tweets in the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T08:36:25.031807Z",
     "start_time": "2021-03-13T08:36:25.028090Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5j3ROjtmvaGW",
    "outputId": "94ea2d56-ec54-4c47-d6d3-d0f4e1aceaf6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
     ]
    }
   ],
   "source": [
    "def processTweet(text):\n",
    "    '''\n",
    "    In this function we tokenize the tweets i.e. separate each word in the tweet\n",
    "    params -\n",
    "    text: Tweet that needs to be tokenized\n",
    "    return -\n",
    "    processed_text: Return the text after tokenizing and stemming\n",
    "    '''\n",
    "    # we use RegExp tokenizer to tokenize the text in the tweet\n",
    "    processed_text = RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "\n",
    "    # we use PorterStemmer to stem each token\n",
    "    processed_text = [PorterStemmer().stem(word) for word in processed_text]\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Apply the processTweet function on every tweet in the train data\n",
    "df_en.tokens = df_en['tweet'].apply(lambda x: processTweet(x))\n",
    "\n",
    "# Apply the processTweet function on every tweet in the test data\n",
    "df_test.tokens = df_test['tweet'].apply(lambda x: processTweet(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0NhL_KFtDii"
   },
   "source": [
    "We have successfully processed and prepared our data for machine learning. Our train and test data is now ready for machine learning.\n",
    "\n",
    "## 5. Create the machine learning model <a class=\"anchor\" id=\"five\"></a>\n",
    "\n",
    "In this step, we create the machine learning model and the TFIDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-13T08:36:38.410001Z",
     "start_time": "2021-03-13T08:36:38.401965Z"
    },
    "id": "rQpo-6TSUCsB"
   },
   "outputs": [],
   "source": [
    "# Get the test and train data\n",
    "X_train, X_test, y_train = df_en.tokens, df_test.tokens, df_en.label\n",
    "\n",
    "# Creating the TFiDF vectoriser form\n",
    "tfidf = TfidfVectorizer(analyzer = \"word\", ngram_range = (1, 2), \n",
    "                        stop_words = 'english', use_idf = True, max_df = 0.95, \\\n",
    "                        min_df = 0.001, sublinear_tf = True)\n",
    "\n",
    "# Fit and transform the data on training data set\n",
    "X_tr_tfidf = tfidf.fit_transform([' '.join(value) for value in X_train])\n",
    "\n",
    "# Fit and transform the data on test data\n",
    "X_test_tfidf = tfidf.transform([' '.join(value) for value in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIuMUBUgueua"
   },
   "source": [
    "We have successfully trained our model and created the TFIDF vector.\n",
    "\n",
    "In the next step, we make prediction.\n",
    "\n",
    "## 6. Classify each tweet in the test data <a class=\"anchor\" id=\"six\"></a>\n",
    "\n",
    "In this step, we make predictions and classify the tweets in the test data into the given categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WQLUTkRZi_8",
    "outputId": "42134049-3eeb-4d3e-d1af-ed10e162cc5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the linear SVC model with params\n",
    "# C, Kernel, degree, gamma\n",
    "model = svm.SVC(C = 1.0, kernel = 'linear', degree = 3, gamma = 'auto')\n",
    "\n",
    "# Fitting the SVC model to the training data\n",
    "model.fit(X_tr_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "l86LwfaDaKjb"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "df_test['label'] = list(model.predict(X_test_tfidf))\n",
    "\n",
    "# save prediction to the prediction.csv file\n",
    "df_test[['Train_id', 'label']].to_csv('prediction.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x-S80tbvEqr"
   },
   "source": [
    "The predictions are saved to the `prediction.csv` file."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "twitter_analysis_acc.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
